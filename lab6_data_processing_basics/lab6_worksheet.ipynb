{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc9477f",
   "metadata": {},
   "source": [
    "# Lab 6 - More Data!\n",
    "\n",
    "Today, we will learn to process each subject's data, aggregate all the subject's data into one data frame, and then visualize the grouped data!\n",
    "\n",
    "To follow along, please download all the materials (including the .csv data files) from the Week 6 Lab folder.\n",
    "\n",
    "## Processing\n",
    "\n",
    "#### Steps: \n",
    "1. **Import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8fcb1",
   "metadata": {},
   "source": [
    "2. **Read in data as a data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sub-101_fa-5_redlight_greenlight.csv'\n",
    "\n",
    "subject_df = pd.read_csv(filename)\n",
    "\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_la = (subject_df[\"resp\"].isna()) & (subject_df[\"trial_type\"] == \"nogo\")\n",
    "\n",
    "cr_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df.loc[hit_la, \"sdt_label\"] = \"hit\"\n",
    "subject_df.loc[cr_la, \"sdt_label\"] = \"cr\"\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dce5b",
   "metadata": {},
   "source": [
    "3. **Test whether each row (i.e., trial) is a \"HIT\", \"FA\", \"MISS\", or \"CR\"**. We will do this by creating logical arrays. To test if a value is NaN, use `.isna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's generate logical arrays for each signal detection label:\n",
    "hit_la = (subject_df[\"resp\"] == \"space\") & (subject_df[\"trial_type\"] == \"go\")\n",
    "fa_la = (subject_df[\"resp\"] == \"space\") & (subject_df[\"trial_type\"] == \"nogo\")\n",
    "cr_la = (subject_df[\"resp\"].isna()) & (subject_df[\"trial_type\"] == \"nogo\")\n",
    "miss_la = (subject_df[\"resp\"].isna()) & (subject_df[\"trial_type\"] == \"go\")\n",
    "\n",
    "# Optionally, add a column called sdt_label, which labels each trial\n",
    "subject_df.loc[hit_la, \"sdt_label\"] = \"hit\"\n",
    "subject_df.loc[fa_la, \"sdt_label\"] = \"fa\"\n",
    "subject_df.loc[cr_la, \"sdt_label\"] = \"cr\"\n",
    "subject_df.loc[miss_la, \"sdt_label\"] = \"miss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df[\"trial_type\"] == 'go'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746585f",
   "metadata": {},
   "source": [
    "4. **Calculate Hit Rate and FA Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245face4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of hits and FAs using the logical arrays\n",
    "hit_count = sum(hit_la)\n",
    "fa_count = sum(fa_la)\n",
    "\n",
    "# Option 2, count the number of hits and FAs using the sdt_label column:\n",
    "\n",
    "\n",
    "# Count the number of nogo trials or go trials\n",
    "go_count = sum(subject_df[\"trial_type\"] == 'go')\n",
    "nogo_count = sum(subject_df[\"trial_type\"] == 'nogo')\n",
    "\n",
    "# Calculate Hit Rate as hit_count/go_count; do same for fa_rate (fa_count/nogo_count)\n",
    "hit_rate = hit_count/go_count\n",
    "fa_rate = fa_count/nogo_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fa8ca",
   "metadata": {},
   "source": [
    "5. **Calculate $d_{prime}$ and $\\lambda$ assuming equal variance Gaussian model**. For this, you need to install and import the `scipy` library. We can use some of the stats methods to easily get the Z values from probabilities. Documentations for the stats methods here: https://scipy.github.io/devdocs/tutorial/stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# Convert the hit and fa rates to Z_hit and Z_fa\n",
    "Z_hit = st.norm.ppf(hit_rate)\n",
    "Z_fa = st.norm.ppf(fa_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lambda (Book Eq'n 2.3)\n",
    "lam = -Z_fa\n",
    "\n",
    "# Calculate d-prime (Book Eq'n 2.4)\n",
    "d_prime = Z_hit - Z_fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a023416",
   "metadata": {},
   "source": [
    "6. **Calculate bias metrics $log\\beta$ and $\\lambda_{center}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lam_center (Book Eq'n 2.5)\n",
    "lam_center = lam - .5*d_prime\n",
    "\n",
    "# What's an alternative what to calculate lam_center from Z_hit and Z_fa? (Book Eq'n 2.6)\n",
    "lam_center = -.5*(Z_fa + Z_hit)\n",
    "\n",
    "# Calculate log_beta (Book Eq'n 2.10)\n",
    "log_beta = d_prime*(lam - .5*d_prime)\n",
    "\n",
    "log_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb1731",
   "metadata": {},
   "source": [
    "Based on these bias metrics, is the subject more biased towards the signal or the noise distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df56f4",
   "metadata": {},
   "source": [
    "7. **Record the all the metrics you just calculated in a list.** Also include Participant ID and the Condition.\n",
    "\n",
    "When dealing with strings, you'll often need to parse and pattern match. This falls into the world of **regular expressions** or **RegEx**. Python has a base regex library called `re` (https://docs.python.org/3/library/re.html). We will use this to grab participant id and condition from `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "print(filename)\n",
    "re.split('',filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a base python library for regular expressions.\n",
    "import re\n",
    "\n",
    "# Extract Participant ID and Condition from the filename\n",
    "_, sub, _, cond, _, _ = re.split('-|_',filename)\n",
    "\n",
    "cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb480a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include sub, cond, and metrics into a list\n",
    "subject_list = [sub, cond, hit_rate, fa_rate, lam, d_prime, lam_center, log_beta]\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2bedc",
   "metadata": {},
   "source": [
    "8. **Now, put all the previous steps in a loop!** Loop through all the data files that you collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608f901",
   "metadata": {},
   "source": [
    "Use glob to find all the _redlight_greenlight.csv files in this directory. This is a simple pattern matching way of retrieving only files that match a certain format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc567c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data_filenames = glob.glob(\"*_redlight_greenlight.csv\")\n",
    "\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e35e22",
   "metadata": {},
   "source": [
    "Simply iterate through each `filename` in `data_filenames` and copy the steps above into the code chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_list = []\n",
    "for filename in data_filenames:\n",
    "    \n",
    "    # Read filename into a data frame\n",
    "    subject_df = pd.read_csv(filename)\n",
    "    \n",
    "    # First, let's generate logical arrays for each signal detection label:\n",
    "    hit_la = (subject_df[\"resp\"] == \"space\") & (subject_df[\"trial_type\"] == \"go\")\n",
    "    fa_la = (subject_df[\"resp\"] == \"space\") & (subject_df[\"trial_type\"] == \"nogo\")\n",
    "    cr_la = (subject_df[\"resp\"].isna()) & (subject_df[\"trial_type\"] == \"nogo\")\n",
    "    miss_la = (subject_df[\"resp\"].isna()) & (subject_df[\"trial_type\"] == \"go\")\n",
    "    \n",
    "    # Count the number of hits and FAs using the logical arrays\n",
    "    hit_count = sum(hit_la)\n",
    "    fa_count = sum(fa_la)\n",
    "\n",
    "    # Count the number of nogo trials or go trials\n",
    "    go_count = sum(subject_df.trial_type == 'go')\n",
    "    nogo_count = sum(subject_df.trial_type == 'nogo')\n",
    "\n",
    "    # Calculate Hit Rate as hit_count/go_count; do same for fa_rate (fa_count/nogo_count)\n",
    "    hit_rate = hit_count/go_count\n",
    "    fa_rate = fa_count/nogo_count\n",
    "    \n",
    "    # Convert the hit and fa rates to Z_hit and Z_fa\n",
    "    Z_hit = st.norm.ppf(hit_rate)\n",
    "    Z_fa = st.norm.ppf(fa_rate)\n",
    "    \n",
    "    # Calculate lambda (Book Eq'n 2.3)\n",
    "    lam = -Z_fa\n",
    "\n",
    "    # Calculate d-prime (Book Eq'n 2.4)\n",
    "    d_prime = Z_hit - Z_fa\n",
    "    \n",
    "    # Calculate lam_center (Book Eq'n 2.6)\n",
    "    lam_center = lam - .5*d_prime\n",
    "\n",
    "    # Calculate log_beta (Book Eq'n 2.10)\n",
    "    log_beta = d_prime*(lam - .5*d_prime)\n",
    "\n",
    "    # Extract Participant ID and Condition from the filename\n",
    "    _, sub, _, cond, _, _ = re.split('-|_',filename)\n",
    "    \n",
    "    # Include sub, cond, and metrics into a list\n",
    "    subject_list = [sub, cond, hit_rate, fa_rate, lam, d_prime, lam_center, log_beta]\n",
    "    \n",
    "    # Append subject_list to aggregated_list\n",
    "    aggregated_list.append(subject_list)\n",
    "\n",
    "aggregated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1268e",
   "metadata": {},
   "source": [
    "We now have an `aggregated_list` which is a nested list that contains all of the signal detection metrics for all our participants. Let's put it into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f50f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put aggregated_list in a data frame, specifying column headers\n",
    "aggregated_df = pd.DataFrame(aggregated_list,\n",
    "                         columns = [\"subject_id\", \"fa_penalty\", \"hit_rate\", \"fa_rate\", \"lambda\", \"d_prime\", \"lambda_center\", \"log_beta\"])\n",
    "\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd222923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this  data frame as a csv\n",
    "aggregated_df.to_csv('aggregated_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad1cbf",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "When it comes to visualizing your data in python, you have *many* options (matplotlib, seaborn, plotly, plotnine). We will use `plotnine` for just a brief demonstration today, because it is based off of `ggplot` packages in R. However, it's more common for pure Python users to learn matplotlib, seaborn, and plotly - so I might cover those later.\n",
    "\n",
    "### Plot Hit Rate and FA Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_point, geom_col, geom_boxplot\n",
    "\n",
    "ggplot(aggregated_df) + aes(x=\"fa_penalty\", y=\"hit_rate\") + geom_boxplot() + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aggregated_df) + aes(x=\"fa_penalty\", y=\"fa_rate\") + geom_boxplot() + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdf9eb",
   "metadata": {},
   "source": [
    "### Plot the bias metrics\n",
    "How might we predict participants to respond when there's a greater FA penalty? Would they press the spacebar more often or less often?\n",
    "\n",
    "Which of our two conditions (5-penalty vs 10-penalty) would have a more positive log_beta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aggregated_df) + aes(x=\"fa_penalty\", y=\"log_beta\") + geom_boxplot() + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea428d2b",
   "metadata": {},
   "source": [
    "# For Next Week:\n",
    "Next week, I want to cover things that are directly relevant to helping you create your experiment. **If there is something relevant to your project that you would like me to go over next week, please e-mail me and I'll try to incorporate it into next week's lesson plan.**\n",
    "\n",
    "Here are a couple of topic ideas:\n",
    "- Simple statistical tests (e.g., correlations, t-tests, anovas)\n",
    "- Should I create a simple experiment using auditory stimuli?\n",
    "- Visual stimuli processing? I can cover how I use these image processing software:\n",
    "    1. XnConvert - great for batch processing (e.g., turn all your images to grayscale; resize; crop; etc.)\n",
    "    2. Photoshop - more involved but sometimes necessary (e.g., isolate foreground object from background)\n",
    "- Auditory stimuli processing? \n",
    "    1. Audacity - lots of features, but I mainly use it to trim audio files or match volume levels between clips\n",
    "- Any other requests are welcome! I will do my best to accomodate.\n",
    "\n",
    "#### In terms of where to get stimuli for your experiment, I have an extensive library of images and some audio stimuli. Here are some that I have:\n",
    "1. Animals\n",
    "2. Tools\n",
    "3. Produce\n",
    "4. Line drawings (Snodgrass and Vanderwart; Nishimoto)\n",
    "5. Emotional images (IAPS)\n",
    "6. BOSS\n",
    "7. OASIS\n",
    "8. Lots of random objects\n",
    "9. Emotional auditory stimuli (IADS)\n",
    "\n",
    "To find images, you can also use google images (although if you were conducting an IRB-approved study, you would be restricted to images with a Creative Commons license). You can use Pixabay and Wikimedia Commons to search for free and open access images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
